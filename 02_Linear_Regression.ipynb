{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Linear Regression\r\n",
    "\r\n",
    "- $H(X)$: Hypothesis of given $x$ value. 주어진 값에 대한 예측\r\n",
    "- $cost(W,b)$: $H(x)$가 y와 일치하는 정도. 얼마나 잘 예측했는지를 표현하는 값."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "import torch.optim as optim"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### What is ```torch.manual_seed()```?!?!\r\n",
    "- 동일 실험 진행을 위해서는 동일한 난수사용이 필요하다. \r\n",
    "```random.rand()```는 항상 다른 난수 집합이 형성되지만, ```manual_seed```를 사용하면 동일한 난수가 형성된다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "torch.manual_seed(1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1731a7535b0>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data\r\n",
    "\r\n",
    "기본적으로 pytorch는 NCHW format을 가지고 있다.\r\n",
    "첫 단계인 사전처리를 실행. 이미지 형식을 NCHW 형식으로 변환한다.\r\n",
    "NCHW는 배치(batch-N), 채널(channals-C), 깊이(depth-D), 높이(height-H), 폭(width-W)를 나타낸다.\r\n",
    "다차원 배열, 데이터프레임, 행렬을 1-D array로 저장하는 방법이다.(다차원을 하나로 \"casting\"하는 방법 중 하나이다.)\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "x_train = torch.FloatTensor([[1],[2],[3]])\r\n",
    "y_train = torch.FloatTensor([[1],[2],[3]])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "print(x_train)\r\n",
    "print(x_train.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "print(y_train)\r\n",
    "print(y_train.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Weight Initialization\r\n",
    "\r\n",
    "```requires_grad = True```는 ```autograd```에 모든 연산을 추적하도록 한다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "W = torch.zeros(1, requires_grad = True)\r\n",
    "print(W)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "b = torch.zeros(1, requires_grad = True)\r\n",
    "print(b)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hypothesis\r\n",
    "\r\n",
    "$H(x) = Wx +b$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "hypothesis =  x_train*W + b\r\n",
    "print(hypothesis)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cost Function\r\n",
    "\r\n",
    "$cost(W,b) = \\frac{1}{m} \\sum^m_{i=1}( H(x^{(i)}) - y^{(i)})^2$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "print(hypothesis)\r\n",
    "print(y_train)\r\n",
    "print(hypothesis - y_train)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<AddBackward0>)\n",
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]])\n",
      "tensor([[-1.],\n",
      "        [-2.],\n",
      "        [-3.]], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "print((hypothesis - y_train)**2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1.],\n",
      "        [4.],\n",
      "        [9.]], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "cost = torch.mean((hypothesis - y_train)**2)\r\n",
    "print(cost)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(4.6667, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Gradient Descent"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "#  최적화(optimizer)를 위한 확률적 경사하강법의 사용(SGD; Stochastic Gradient Descent)\r\n",
    "optimizer = optim.SGD([W,b],lr=0.01) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "optimizer.zero_grad()\r\n",
    "cost.backward() #현재 tensor의 기울기를 계산\r\n",
    "optimizer.step() #single optimization step 수행"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "print(W)\r\n",
    "print(b)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([0.0933], requires_grad=True)\n",
      "tensor([0.0400], requires_grad=True)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "#hypothesis 비교\r\n",
    "\r\n",
    "hypothesis = x_train * W + b\r\n",
    "print(hypothesis)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.1333],\n",
      "        [0.2267],\n",
      "        [0.3200]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "cost = torch.mean((hypothesis - y_train)**2)\r\n",
    "print(cost)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(3.6927, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Full-code\r\n",
    "\r\n",
    " 간단한 loop의 사용을 통해 다수의 epochs으로 학습을 진행 할 수 있다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "#Data\r\n",
    "x_train = torch.FloatTensor([[1],[2],[3]])\r\n",
    "y_train = torch.FloatTensor([[1],[2],[3]])\r\n",
    "\r\n",
    "#Model Initialization\r\n",
    "W = torch.zeros(1,requires_grad=True)\r\n",
    "b = torch.zeros(1,requires_grad=True)\r\n",
    "\r\n",
    "#Set optimizer\r\n",
    "optimizer = optim.SGD([W,b], lr=0.01)\r\n",
    "\r\n",
    "nb_epochs = 1000\r\n",
    "\r\n",
    "for epoch in range(nb_epochs+1):\r\n",
    "\r\n",
    "    hypothesis = x_train * W + b\r\n",
    "    cost = torch.mean((hypothesis - y_train)**2)\r\n",
    "\r\n",
    "    optimizer.zero_grad()\r\n",
    "    cost.backward()\r\n",
    "    optimizer.step()\r\n",
    "\r\n",
    "    if epoch %100 == 0:\r\n",
    "        print('Epoch {:4d}/{} W = {:.3f}, b= {:.3f}, cost: {:.6f}'.format(epoch, nb_epochs, W.item(), b.item(), cost.item()))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch    0/1000 W = 0.093, b= 0.040, cost: 4.666667\n",
      "Epoch  100/1000 W = 0.873, b= 0.289, cost: 0.012043\n",
      "Epoch  200/1000 W = 0.900, b= 0.227, cost: 0.007442\n",
      "Epoch  300/1000 W = 0.921, b= 0.179, cost: 0.004598\n",
      "Epoch  400/1000 W = 0.938, b= 0.140, cost: 0.002842\n",
      "Epoch  500/1000 W = 0.951, b= 0.110, cost: 0.001756\n",
      "Epoch  600/1000 W = 0.962, b= 0.087, cost: 0.001085\n",
      "Epoch  700/1000 W = 0.970, b= 0.068, cost: 0.000670\n",
      "Epoch  800/1000 W = 0.976, b= 0.054, cost: 0.000414\n",
      "Epoch  900/1000 W = 0.981, b= 0.042, cost: 0.000256\n",
      "Epoch 1000/1000 W = 0.985, b= 0.033, cost: 0.000158\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Using ```nn.Module```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "#Data\r\n",
    "x_train_ = torch.FloatTensor([[1],[2],[3]])\r\n",
    "y_train_ = torch.FloatTensor([[1],[2],[3]])\r\n",
    "\r\n",
    "\r\n",
    "#build linear regression model\r\n",
    "class LinearRegressionModel(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super().__init__()\r\n",
    "        self.linear = nn.Linear(1,1)\r\n",
    "\r\n",
    "    def forward(self,x):\r\n",
    "        return self.linear(x)\r\n",
    "\r\n",
    "\r\n",
    "model = LinearRegressionModel()\r\n",
    "\r\n",
    "#Hypothesis \r\n",
    "#Hypothesis > return forward of nn.Module\r\n",
    "hypothesis_ = model(x_train)\r\n",
    "print(list(model.parameters()))\r\n",
    "print(hypothesis_)\r\n",
    "\r\n",
    "#Cost\r\n",
    "print(hypothesis_)\r\n",
    "print(y_train)\r\n",
    "\r\n",
    "cost = F.mse_loss(hypothesis_, y_train) #MSE(Mean Squared Error) PyTorch에서 제공\r\n",
    "print(cost)\r\n",
    "\r\n",
    "#Gradient Descent\r\n",
    "optimizer_ = optim.SGD(model.parameters(),lr=0.01)\r\n",
    "optimizer.zero_grad()\r\n",
    "cost.backward()\r\n",
    "optimizer.step()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.2774]], requires_grad=True), Parameter containing:\n",
      "tensor([0.0493], requires_grad=True)]\n",
      "tensor([[0.3267],\n",
      "        [0.6041],\n",
      "        [0.8814]], grad_fn=<AddmmBackward>)\n",
      "tensor([[0.3267],\n",
      "        [0.6041],\n",
      "        [0.8814]], grad_fn=<AddmmBackward>)\n",
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]])\n",
      "tensor(2.2968, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "# Using loop\r\n",
    "\r\n",
    "x_train_=torch.FloatTensor([[1],[2],[3]])\r\n",
    "y_train_=torch.FloatTensor([[1],[2],[3]])\r\n",
    "\r\n",
    "model=LinearRegressionModel()\r\n",
    "\r\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.01)\r\n",
    "nb_epochs = 1000\r\n",
    "\r\n",
    "for epoch in range(nb_epochs+1):\r\n",
    "\r\n",
    "    #hypothesis\r\n",
    "    pred=model(x_train_)\r\n",
    "\r\n",
    "    #cost\r\n",
    "    cost=F.mse_loss(pred,y_train_)\r\n",
    "\r\n",
    "    #optimize the cost\r\n",
    "    optimizer.zero_grad()\r\n",
    "    cost.backward()\r\n",
    "    optimizer.step()\r\n",
    "\r\n",
    "    if epoch%100==0:\r\n",
    "        para = list(model.parameters())\r\n",
    "        w = para[0].item()\r\n",
    "        b = para[1].item()\r\n",
    "\r\n",
    "        print('Epoch {:4d}/{} w:{:.3f} b:{:.3f} Cost:{:.6f}'.format(epoch,nb_epochs,w,b,cost.item()))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch    0/1000 w:0.031 b:-0.045 Cost:5.766424\n",
      "Epoch  100/1000 w:0.890 b:0.251 Cost:0.009079\n",
      "Epoch  200/1000 w:0.913 b:0.197 Cost:0.005610\n",
      "Epoch  300/1000 w:0.932 b:0.155 Cost:0.003467\n",
      "Epoch  400/1000 w:0.946 b:0.122 Cost:0.002142\n",
      "Epoch  500/1000 w:0.958 b:0.096 Cost:0.001324\n",
      "Epoch  600/1000 w:0.967 b:0.075 Cost:0.000818\n",
      "Epoch  700/1000 w:0.974 b:0.059 Cost:0.000505\n",
      "Epoch  800/1000 w:0.980 b:0.047 Cost:0.000312\n",
      "Epoch  900/1000 w:0.984 b:0.037 Cost:0.000193\n",
      "Epoch 1000/1000 w:0.987 b:0.029 Cost:0.000119\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.9 64-bit"
  },
  "interpreter": {
   "hash": "c44dac2f6a0ea984347c1934da8545d63431357f8f096634b776fd0ed6ad97ed"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}